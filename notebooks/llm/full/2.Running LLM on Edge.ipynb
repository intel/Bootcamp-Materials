{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26882f6-695d-49c5-8126-c4217423235c",
   "metadata": {},
   "source": [
    "# Running LLM on EDGE - Inferencing with Intel® CPU / GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730e2b5-d267-4f2d-a852-d78cb05c7410",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ccbc4-49e5-403a-8264-7134001018b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Framework\n",
    "import torch\n",
    "\n",
    "# Intel Extension for PyTorch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# BigDL LLM - Transformers Wrapper\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "# Transformers API\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f04a7-7a7b-42c3-b093-37bcfaf7322b",
   "metadata": {},
   "source": [
    "### Load model from Hugging Face\n",
    "\n",
    "In many cases, the architecture of the model that you want to use can be guessed from the name or path of the pretrained model that you are supplying through *from_pretrained()*.\n",
    "Instantiating one of *AutoConfig*, *AutoModel*, *AutoTokenizer* will directly create a class of relevant architecture. For Example:\n",
    "\n",
    "```python\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "```\n",
    "\n",
    "Below is an example of using specific task AutoModel - AutoModelForCausalLM\n",
    "**AutoModelForCausalLM** - a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) [link](https://huggingface.co/docs/transformers/model_doc/auto#natural-language-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178fe52-1284-4a21-889d-8aa3ec5523b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit=\"sym_int4\", optimize_model=True, trust_remote_code=True, use_cache=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa1133b2-bf32-47c7-9b86-a3d84787a343",
   "metadata": {},
   "source": [
    "### Initiate tokenizer from the model\n",
    "\n",
    "Tokenizer is a model that splits input and output texts into smaller units called tokens. These tokens can be words, characters, subwords, or symbols, depending on the type and size of the model.\n",
    "The tokens (integers) then is used to predict which tokens should come next\n",
    "\n",
    "<div style=\"margin-top: 20px\">\n",
    "<img src=\"./imgs/gpt-token-encoder-decoder.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "[Reference: Understanding GPT tokenizers](https://www.google.com/url?sa=i&url=https%3A%2F%2Fsimonwillison.net%2F2023%2FJun%2F8%2Fgpt-tokenizers%2F&psig=AOvVaw0_BgjL6lj6CIT_CvjmkWsR&ust=1700270023806000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCMjylMjtyYIDFQAAAAAdAAAAABAJ)\n",
    "\n",
    "**Note**: Every model uses it's own architecture tokenizer. It is best to load the tokenizer from the model itself\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb5188-7e7b-47b6-a730-1ac9a1bd50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd48b64-dae2-4270-93da-ed69c65bef9e",
   "metadata": {},
   "source": [
    "### Move model to specific xPU (CPU / GPU) accelerator\n",
    "Once the model has been initialized, it can be move to specific accelerator\n",
    "\n",
    "One could easily check the available devices using ipex api:\n",
    "```python\n",
    "for i in range(torch.xpu.device_count()):\n",
    "    device_name = torch.xpu.get_device_name(i)\n",
    "    print(f\"{i}: {device_name}\")\n",
    "``` \n",
    "\n",
    "List of available xPU devices available in the system *(defer for every machine)*\n",
    "```\n",
    "0: Intel(R) Arc(TM) A770 Graphics\n",
    "1: Intel(R) Arc(TM) A770 Graphics\n",
    "2: Intel(R) UHD Graphic770\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d10a90-7b5e-41f4-9ea7-32e88d32a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.xpu.device_count()):\n",
    "    device_name = torch.xpu.get_device_name(i)\n",
    "    print(f\"{i}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f5570-d412-4e0f-90bb-ffed5efc2a20",
   "metadata": {},
   "source": [
    "### Select the target xPU to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38246fd7-0924-4098-ab81-14e84445048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4f685-a769-49ed-889d-2bc264615461",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "Prompt templates are predefined structures or formats that guide users in providing input to a language model. Templates help in improving the clarity, consistency, and the overall quality of LLM responses\n",
    "\n",
    "Prompt format varies from model to model. Constantly check on the model's page for the prompt template format:\n",
    "\n",
    "Eg: [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format)\n",
    "\n",
    "```\n",
    "text = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n",
    "\"[INST] Do you have mayonnaise recipes? [/INST]\"\n",
    "```\n",
    "\n",
    "Eg: [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT_en.d)\n",
    "\n",
    "```\n",
    "<|system|>\n",
    "You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\n",
    "<|user|>\n",
    "Hello\n",
    "<|assistant|>\n",
    "Hello, I'm ChatGLM3. What can I assist you today?\n",
    "```\n",
    "\n",
    "**Note**: Always refer to the model page for prompt template format\n",
    "\n",
    "**Note**: Prompt template is defined by model's creator during model training / fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f5598-1274-4fc4-a77c-3d7e8b1968a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_PROMPT_FORMAT = \"\"\"<s>[INST]{prompt}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a5a89-b86c-4a30-8eca-c0ddb9dc5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "input_prompt = widgets.Text(\n",
    "    value='Did you encounter an error message saying \"Legacy-Install-Failure\" when installing the Openvino-Dev pip package using Python 3.10?',\n",
    "    placeholder='Type something',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"90%\")\n",
    ")\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c916a-3189-466c-99fc-d0e9da0d6bf2",
   "metadata": {},
   "source": [
    "### Run inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27954bb2-c11d-4e79-bd14-87807bc4bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "        # prompt = MISTRAL_PROMPT_FORMAT.format(prompt=\"Two things are infinite\")\n",
    "        prompt = MISTRAL_PROMPT_FORMAT.format(prompt=input_prompt.value)\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(input_ids, temperature=0.3, do_sample=True, max_new_tokens=32, use_cache=True)\n",
    "        torch.xpu.synchronize()\n",
    "        output = output.cpu()\n",
    "        output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print('-'*20, 'Output', '-'*20)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa99cc",
   "metadata": {},
   "source": [
    "## Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation. \n",
    "\n",
    "No product or component can be absolutely secure.  \n",
    "\n",
    "Your costs and results may vary.  \n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (0BSD), Open Source Initiative. No rights are granted to create modifications or derivatives of this document. \n",
    "\n",
    "© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
