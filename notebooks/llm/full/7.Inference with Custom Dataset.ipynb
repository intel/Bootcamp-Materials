{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18292183-dcbd-4197-9162-ff9024e4955a",
   "metadata": {},
   "source": [
    "# Inference With Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7495d7-38ea-4cac-844d-da511af5e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78c0c9-e2e8-4582-943e-f26b56584669",
   "metadata": {},
   "source": [
    "## Inference on Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8d7fe-c7ef-4255-850f-a719eb749455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "input_prompt = widgets.Text(\n",
    "    value='Did you encounter an error message saying \"Legacy-Install-Failure\" when installing the Openvino-Dev pip package using Python 3.10?',\n",
    "    placeholder='Type something',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"90%\")\n",
    ")\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcbc8c-23c1-42f8-bd45-218d143e0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT = \"### Question: {prompt} ### Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531783ce-a827-4da9-ac49-0ac7160c0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"xpu\"\n",
    "model_path = \"./outputs/own-merged-llm\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, optimize_model=True, trust_remote_code=True)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "with torch.inference_mode():\n",
    "    prompt = CUSTOM_PROMPT.format(prompt=input_prompt.value)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, temperature=0.1, max_new_tokens=512)\n",
    "    if device == \"xpu\":\n",
    "        torch.xpu.synchronize()\n",
    "    output = output.cpu()\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output_str)\n",
    "    finetuned_result = output_str.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa4f1e-45b9-4b7d-a51f-7a7a18b82121",
   "metadata": {},
   "source": [
    "## Inference on Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0500b-606d-41eb-9dfc-baea5040a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer\n",
    "torch.xpu.empty_cache()\n",
    "\n",
    "device = \"xpu\"\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "ori_model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, optimize_model=True, trust_remote_code=True)\n",
    "ori_model = ori_model.to(device)\n",
    "ori_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "with torch.inference_mode():\n",
    "    prompt = CUSTOM_PROMPT.format(prompt=input_prompt.value)\n",
    "    input_ids = ori_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = ori_model.generate(input_ids, temperature=0.1, max_new_tokens=512)\n",
    "    if device == \"xpu\":\n",
    "        torch.xpu.synchronize()\n",
    "    output = output.cpu()\n",
    "    output_str = ori_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output_str)\n",
    "    original_result = output_str.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61bc4f-e865-4910-b582-2bebd8c3c0f7",
   "metadata": {},
   "source": [
    "## Original Result vs Fine-tuned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f200449-5af9-4603-a544-884035f02aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "==================================== Original Model =============================================\n",
    "{original_result}\n",
    "===================================== Fine-Tuned Model ===========================================\n",
    "{finetuned_result}\n",
    "=================================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785fd40",
   "metadata": {},
   "source": [
    "## Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation. \n",
    "\n",
    "No product or component can be absolutely secure.  \n",
    "\n",
    "Your costs and results may vary.  \n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (0BSD), Open Source Initiative. No rights are granted to create modifications or derivatives of this document. \n",
    "\n",
    "© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
