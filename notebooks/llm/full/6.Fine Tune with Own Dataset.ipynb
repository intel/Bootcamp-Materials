{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13e0275-9874-46e9-8940-2f7449569d30",
   "metadata": {},
   "source": [
    "# Fine-tune with Own Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee36923-2042-46ee-95eb-268bfd205a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from bigdl.llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ed00b-3ac7-4200-ad57-5ed9931f1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf outputs/own-lora-model\n",
    "!rm -rf outputs/own-merged-llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0feba66-e33e-4813-b4ba-95fb46f5027f",
   "metadata": {},
   "source": [
    "<img src=\"imgs/finetune_pipe.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d73e5-47d8-446b-accc-bcd6f179063c",
   "metadata": {},
   "source": [
    "## Load Desired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4b6dc-8565-4717-881c-979f470d6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit=\"nf4\", optimize_model=False, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b07f0-3e30-41c7-bba6-5cf81d4b1c6a",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a169b-e460-42b9-856d-1244450e59b9",
   "metadata": {},
   "source": [
    "*In the context of natural language processing and machine learning, **pad_token_id** typically refers to the identifier or index assigned to a special token representing padding in a sequence. When working with sequences of varying lengths, it's common to pad shorter sequences with a special token to make them uniform in length.*\n",
    "\n",
    "Eg:\n",
    "```\n",
    "data = [ \"I like cat\", \"Do you like cat too?\"]\n",
    "tokenizer(data, padding=True, truncation=True, return_tensors='pt')\n",
    "```\n",
    "Output:\n",
    "```\n",
    "'input_ids': tensor([[101,146,1176,5855,102,0,0,0],[101,2091,1128,1176,5855,1315,136,102]])\n",
    "'token_type_ids': tensor([[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0]])\n",
    "'attention_mask': tensor([[1,1,1,1,1,0,0,0],[1,1,1,1,1,1,1,1]])\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825a153-80a1-41ae-b44f-083721206e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=True, max_model_length=512, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = 0 \n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778cd97-0362-432c-9cf2-a32f67214224",
   "metadata": {},
   "source": [
    "## Load Own Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e8c33-5cfb-43b7-8e55-e71d4622aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "with open('outputs/custom_data.pickle', 'rb') as handle:\n",
    "    custom_data = pickle.load(handle)\n",
    "\n",
    "custom_prompt_format = \"### Question: {question} ### Answer: {answer}\"\n",
    "\n",
    "def process_data(sample):\n",
    "    prompt = custom_prompt_format.format(question=sample[\"question\"], answer=sample[\"answer\"])\n",
    "    print(prompt)\n",
    "    return tokenizer(prompt)\n",
    "\n",
    "dataset = Dataset.from_list(custom_data)\n",
    "# dataset = concatenate_datasets([dataset, dataset])\n",
    "dataset = dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4cc57-6b32-4238-8809-5b09774cc886",
   "metadata": {},
   "source": [
    "## Select xPU accelerator for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979773d7-3ab9-4952-a5d6-00df0e1e88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('xpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5ecf6-1954-4d4b-b441-9fb9f6cb2047",
   "metadata": {},
   "source": [
    "## Prepare Model for QLoRA INT4 Fine Tuning\n",
    "\n",
    "<img src=\"imgs/qlora.png\" width=\"800\"/>\n",
    "\n",
    "[Reference](https://www.linkedin.com/pulse/trends-llms-qlora-efficient-finetuning-quantized-vijay/?trk=article-ssr-frontend-pulse_more-articles_related-content-card)\n",
    "\n",
    "Summary:\n",
    "1. 4-bit quantization of the full pretrained language model to compress weights and reduce memory requirements using a novel NormalFloat encoding optimized for the distribution of neural network weights.\n",
    "2. Low-rank adapters added densely throughout the layers of the 4-bit quantized base model. The adapters use full 16-bit precision and are finetuned while the base model remains fixed.\n",
    "3. Double quantization further reduces memory by quantizing the first-stage quantization constants themselves from 32-bit to 8-bit with no accuracy loss.\n",
    "4. Paged optimizers leverage unified memory to gracefully handle gradient checkpointing memory spikes and finetune models larger than the physical GPU memory through automatic paging.\n",
    "5. Mixed precision combines 4-bit weights with 16-bit adapter parameters and activations, maximizing memory savings while retaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eaddb-ff27-410e-bd0a-bf9280713521",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93802cf-b4c9-4e7c-b367-375d4b9bb78e",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "LoraConfig allows you to control how LoRA is applied to the base model through the following parameters:\n",
    "\n",
    "***target_modules** - The modules (for example, attention blocks) to apply the LoRA update matrices* [Reference](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a344f-3c94-4fb1-86bd-3de6b2484c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2495e8-ed62-4d82-8953-31b42cfd7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=20,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-5,\n",
    "        save_steps=100,\n",
    "        bf16=True,  # bf16 is more stable in training\n",
    "        logging_steps=20,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"adamw_hf\", # paged_adamw_8bit is not supported yet\n",
    "        # gradient_checkpointing=True, # can further reduce memory but slower\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "result = trainer.train()\n",
    "model.save_pretrained(\"outputs/own-lora-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85e918-7763-4a2a-a2c7-01fc0b3b5a4d",
   "metadata": {},
   "source": [
    "## Merge LoRA Adapter with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0a7ab-dc87-4f26-a8de-dd4d8a06af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from bigdl.llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313f688-d13e-4a01-a306-b3f0e216c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "adapter_path = \"outputs/own-lora-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        # load_in_low_bit=\"nf4\", # should load the orignal model\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": \"cpu\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46521f8-76b3-427b-8dea-cbb34cafa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        adapter_path,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        torch_dtype=torch.float16,\n",
    ")\n",
    "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d4187-b8ee-4faf-ade7-e0275ae054b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.merge_and_unload()\n",
    "lora_model.train(False)\n",
    "lora_model_sd = lora_model.state_dict()\n",
    "deloreanized_sd = { k.replace(\"base_model.model.\", \"\"): v for k, v in lora_model_sd.items() if \"lora\" not in k }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030cf5ec-e8b5-4f53-a1b9-6094bc6c383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_pretrained(\"outputs/own-merged-llm\", state_dict=deloreanized_sd)\n",
    "tokenizer.save_pretrained(\"outputs/own-merged-llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a318f6",
   "metadata": {},
   "source": [
    "## Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation. \n",
    "\n",
    "No product or component can be absolutely secure.  \n",
    "\n",
    "Your costs and results may vary.  \n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (0BSD), Open Source Initiative. No rights are granted to create modifications or derivatives of this document. \n",
    "\n",
    "© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
