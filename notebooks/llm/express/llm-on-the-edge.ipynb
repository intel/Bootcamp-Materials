{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47d5e53",
   "metadata": {},
   "source": [
    "# Running Large Language Model at the Edge\n",
    "By utilizing the Intel® OneAPI Toolkit and Intel® BigDL-LLM python packages, customers can run LLM on their workstations / personal PCs with optimized performance and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e9a0",
   "metadata": {},
   "source": [
    "## Verifying Python3 path and version\n",
    "Currently BigDL-LLM package is supported on Python3.9. So, the following steps are the sanity check for Python3 version. It is also recommended to use Anaconda as it provides an easier way to create a Python3 virtual environment with the Python version specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e45f84-fdb3-41d0-8d8d-166ba6aecd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c840da",
   "metadata": {},
   "source": [
    "## Installing BigDL-LLM Package\n",
    "Installing BigDL-LLM package is easy as it is a one line installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf3533-8e10-484f-9f7b-acfa10098b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "# !python3 -m pip install -q bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3dd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intel(R) Discrete GPU\n",
    "# !python3 -m pip install --pre --upgrade bigdl-llm[xpu] -f https://developer.intel.com/ipex-whl-stable-xpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6378cdd",
   "metadata": {},
   "source": [
    "## Specifying the LLM prompt, model and prediction token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee1863-3908-42af-b66f-86e2d69a3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_PROMPT_FORMAT = \"\"\"### HUMAN:\n",
    "{prompt}\n",
    "\n",
    "### RESPONSE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a0b8c-2eb6-4cbe-990f-3c850da7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93\"\n",
    "n_predict = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f7ab7",
   "metadata": {},
   "source": [
    "## Running a LLAMA2-7B LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7584a01",
   "metadata": {},
   "source": [
    "### Import the necessary function and class from Python3 transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86fed5-5b74-458a-9183-7711ba2385d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# This will create a models folder on the local path, download and save the model locally\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./models\"\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611aa89",
   "metadata": {},
   "source": [
    "### Create the tokenizers for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef802d3d-b1ed-41a2-a050-e1dadbcd5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16384a44-fa6f-4514-96ac-8255905bf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(prompt, tokenizer, model, streamer):\n",
    "    _prompt = None\n",
    "    optimized_elapsed_time = None\n",
    "    with torch.inference_mode():\n",
    "            _prompt = LLAMA2_PROMPT_FORMAT.format(prompt=prompt)\n",
    "            input_ids = tokenizer.encode(_prompt, return_tensors=\"pt\")\n",
    "            start_time = time.time()\n",
    "            output = model.generate(input_ids, streamer=streamer, max_new_tokens=n_predict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Inference time: {round(elapsed_time, 2)} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ce8ff",
   "metadata": {},
   "source": [
    "### Create the model with Intel® Software Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfab39-5e18-4fad-b21d-cfb624836a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223675d-7e68-4f15-b8b9-491ed089d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1a208-1889-44bc-844e-b57df1e7ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How is your day today?\"\n",
    "generate_output(prompt=prompt, tokenizer=tokenizer, model=model, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6734f6",
   "metadata": {},
   "source": [
    "## Role Prompt with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabc3cb-1cd3-4ead-9e97-3fefdf598bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = \"As a retail store customer support, you are tasked with providing assistance and information to customers visiting your online store. Reply to the question below\"\n",
    "question = \"How can I buy the item online?\"\n",
    "prompt = f\"{sys_msg}\\n### QUESTION: {question}\\n\"\n",
    "generate_output(prompt=prompt, tokenizer=tokenizer, model=model, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f6627",
   "metadata": {},
   "source": [
    "## Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation. \n",
    "\n",
    "No product or component can be absolutely secure.  \n",
    "\n",
    "Your costs and results may vary.  \n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (0BSD), Open Source Initiative. No rights are granted to create modifications or derivatives of this document. \n",
    "\n",
    "© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
