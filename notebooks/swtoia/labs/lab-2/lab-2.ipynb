{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lab 2**\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "This workshop demonstrates basic CUDA to SYCL conversion using SYCLomatic tool and compilation of the SYCL program using \n",
    "Intel® oneAPI DPC++/C++ Compiler.\n",
    "\n",
    "You will learn about SYCL C++ code skeleton and construct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction of SYCL™ C++**\n",
    "\n",
    "To support data/compute parallelism in C++ whereby software programs have access to parallel computing resources in modern heterogenous system (combination of CPU, GPU, FPGA and accelerators), Khronos group has SYCL™ initiative which is an industry-driven standard body that adds data parallelism to C++ which is not fully supported by ISO C++ standard. **The objective of SYCL is to influence the direction of C++ standard for supporting heterogenous compute.**\n",
    "\n",
    "A SYCL program is single-source and contains code sections (known as **host code** for orchestrating data movement and compute offload to devices) that are executed on host CPU and code sections (known as **device kernels** and typically computation workload) that are dispatched to be executed on **SYCL devices** (CPU, GPU, FPGA and other). \n",
    "\n",
    "A SYCL host code uses **SYCL constructs and classes** to organize the parallel computation in heterogenous system and **SYCL-aware C++ compiler** will translate the SYCL source code into respective binaries targeting the underlying devices delivering overall good performance through parallel data computation.\n",
    "\n",
    "**oneAPI DPC++/C++ Compiler** is Intel® distribution of SYCL-aware C++ compiler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of SYCL C++ Program**\n",
    "\n",
    "![SYCL C++ Program Skeleton](./images/sycl-cpp-skeleton.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Verify availability of SYCL devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List SYCL devices on the system\n",
    "! sycl-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the DPC++ Compiler version\n",
    "! icx --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check SYCLomatic tool version\n",
    "! c2s --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Prepare CUDA source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile vectoradd.cu\n",
    "\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#define N 16\n",
    "\n",
    "//# kernel code to perform VectorAdd on GPU\n",
    "__global__ void VectorAddKernel(float* A, float* B, float* C)\n",
    "{\n",
    "        C[threadIdx.x] = A[threadIdx.x] + B[threadIdx.x];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "        //# Initialize vectors on host\n",
    "        float A[N] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n",
    "        float B[N] = {2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2};\n",
    "        float C[N] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "        //# Allocate memory on device\n",
    "        float *d_A, *d_B, *d_C;\n",
    "        cudaMalloc(&d_A, N*sizeof(float));\n",
    "        cudaMalloc(&d_B, N*sizeof(float));\n",
    "        cudaMalloc(&d_C, N*sizeof(float));\n",
    "\n",
    "        //# copy vector data from host to device\n",
    "        cudaMemcpy(d_A, A, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(d_B, B, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "        //# sumbit task to compute VectorAdd on device\n",
    "        VectorAddKernel<<<1, N>>>(d_A, d_B, d_C);\n",
    "\n",
    "        //# copy result of vector data from device to host\n",
    "        cudaMemcpy(C, d_C, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        //# print result on host\n",
    "        for (int i = 0; i < N; i++) std::cout<< C[i] << \" \";\n",
    "        std::cout << \"\\n\";\n",
    "\n",
    "        //# free allocation on device\n",
    "        cudaFree(d_A);\n",
    "        cudaFree(d_B);\n",
    "        cudaFree(d_C);\n",
    "        return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:**\n",
    "* **VectorAddKernel() is CUDA kernel code** that performs parallelized addition of A + B and stores the result to C.\n",
    "* Note that the addition operation of these A+B values can be performed independently.\n",
    "* **float A[N], B[N], C[N]** are floating point arrays on **CPU (host) memory**.\n",
    "* **float *d_A, *d_B, *d_C** are floating point pointers to **GPU (device) memory**.\n",
    "* GPU device memory is allocated using **cudaMalloc()**.\n",
    "* The floating point values on **CPU memory (A[N] and B[N]) are copied to GPU memory** at location pointed by d_A and d_B pointers using **cudaMemcpy()**. \n",
    "* **VectorAddKernel<<<1,N>>>(d_A, d_B, d_C) is how CUDA kernel is submitted to GPU device** in parallelized fashion. In CUDA program, the parallelized addition of \"A+B\" is offloaded into GPU execution units that are executed simulateneously.\n",
    "* The **result of the addition in GPU device memory (d_C) is copied to CPU memory (C[N])**.\n",
    "* Finally, the device memory allocated earlier (d_A, d_B and d_C) are freed.\n",
    "* The CUDA compiler understands CUDA-specific constructs (e.g, <<<1,N>>> and \\_\\_global\\_\\_) and APIs (e.g. cudaMalloc(), cudaMemcpy() and cudaFree()).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use SYCLomatics tool to convert CUDA code to SYCL C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! c2s vectoradd.cu --cuda-include-path=/usr/local/cuda-12.1/include --out-root=sycl_output --gen-helper-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:**\n",
    "* --cuda-include-path=<path to CUDA include>: Specify the CUDA include header path.\n",
    "* --out-root=<SYCL output directory>: Specify the SYCL code output\n",
    "* --gen-helper-function : Generate SYCLomatic helper header files to output \n",
    "\n",
    "**Note:**\n",
    "* oneAPI Base Toolkit version 2023.02 supports CUDA Toolkit version 12.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Review the SYCL output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! tree sycl_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:**\n",
    "* MainSourceFile.yaml – CUDA to SYCL conversion log\n",
    "* vectoradd.dp.cpp – the converted SYCL C++ source code\n",
    "* include/ – SYCLomatic generated SYCL helper headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the converted CUDA converted code (in SYCL C++ code) \n",
    "! cat sycl_output/vectoradd.dp.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:**\n",
    "\n",
    "![SYCL C++ Conversion 1](./images/sycl-convert-1.jpg)\n",
    "![SYCL C++ Conversion 1](./images/sycl-convert-2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Edit SYCL code to print SYCL device information\n",
    "\n",
    "Add following code to sycl_output/vectoradd.dp.cpp as indicated by '+' as follow:\n",
    "```c\n",
    "        //# copy result of vector data from device to host\n",
    "        q_ct1.memcpy(C, d_C, N * sizeof(float)).wait();\n",
    "\n",
    "+        // Print SYCL-device info\n",
    "+        std::cout << \"Running on device: \"\n",
    "+                  << q_ct1.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "        \n",
    "        //# print result on host\n",
    "        for (int i = 0; i < N; i++) std::cout<< C[i] << \" \";\n",
    "        std::cout << \"\\n\";\n",
    "```\n",
    "**Note:**\n",
    "* From Jupyter Notebook, navigate to the file from the LHS file explorer, double-click on the selected file.\n",
    "* Edit the file as shown below and save the change by Ctrl+s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Compile SYCL code using DPC++ compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd sycl_output && icpx -fsycl -I include vectoradd.dp.cpp -o vectoradd_prog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Run the executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd sycl_output && file vectoradd_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd sycl_output && ./vectoradd_prog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Running SYCL program on different SYCL devices\n",
    "\n",
    "The execution of SYCL device/kernel code can be influenced by using **ONEAPI_DEVICE_SELECTOR=\\<option\\>**.\n",
    "The value for the \\<option\\> can be obtained from the output of 'sycl-ls' command.\n",
    "\n",
    "**Note:** For more information about ONEAPI_DEVICE_SELECTOR environment variable, please refer to https://intel.github.io/llvm-docs/EnvironmentVariables.html#sycl-device-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! sycl-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd sycl_output && ONEAPI_DEVICE_SELECTOR=opencl:gpu ./vectoradd_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cd sycl_output && ONEAPI_DEVICE_SELECTOR=opencl:cpu ./vectoradd_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd sycl_output && ONEAPI_DEVICE_SELECTOR=opencl:cpu,gpu ./vectoradd_prog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information:**\n",
    "* If ONEAPI_DEVICE_SELECTOR environment variable is not used, SYCL program will default to SYCL device that gives the best computation performance.\n",
    "* Setting ONEAPI_DEVICE_SELECTOR to a specific value limits the SYCL program to be executed on the specified SYCL device.\n",
    "* For ONEAPI_DEVICE_SELECTOR=opencl:cpu,gpu, SYCL program may be executed on CPU if GPU device is absent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion:**\n",
    "\n",
    "* CUDA program runs on NVIDIA GPU in parallel fashion over a pool of Execution Units (Streaming Multi-processor).\n",
    "* SYCL C++ program can run in CPU, GPU, FPGA and other accelerator.\n",
    "* The conversion from CUDA to SYCL C++ source code can be accelerated using SYCLomatic tool.\n",
    "* A SYCL C++ code is then compiled using Intel oneAPI DPC++/C++ Compiler.\n",
    "* A specific version of SYCLomatic tool (bundled in Intel oneAPI Base Toolkit) support up-to a specific CUDA Toolkit version. (Lagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notices & Disclaimers** \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation. \n",
    "\n",
    "No product or component can be absolutely secure.  \n",
    "\n",
    "Your costs and results may vary.  \n",
    "\n",
    "No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (0BSD), [Open Source Initiative](https://opensource.org/licenses/0BSD). No rights are granted to create modifications or derivatives of this document. \n",
    "\n",
    "© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
